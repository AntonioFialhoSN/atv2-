{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7y3Q97MRdW8OpCfuqa440",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntonioFialhoSN/atv2-/blob/main/atv2_python_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#atv2_python_regression\n",
        "##Functions\n",
        "------------------------------------------------------\n",
        "###compute_cost.py"
      ],
      "metadata": {
        "id": "-XXstJA-u4rO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dDd4vhBvuqqj"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "@file compute_cost.py\n",
        "@brief Computes the cost for linear regression.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def compute_cost(X, y, theta):\n",
        "    \"\"\"\n",
        "    Compute the cost for linear regression.\n",
        "\n",
        "    This function calculates the mean squared error cost function J(θ) for linear regression:\n",
        "    J(θ) = (1 / (2 * m)) * Σ (h(θ) - y)^2\n",
        "\n",
        "    where:\n",
        "    - J(θ) is the cost\n",
        "    - m is the number of training examples\n",
        "    - h(θ) is the hypothesis function (X @ theta)\n",
        "    - y is the vector of observed values\n",
        "\n",
        "    @param X: np.ndarray\n",
        "        Feature matrix including the intercept term (shape: m x n).\n",
        "    @param y: np.ndarray\n",
        "        Target variable vector (shape: m,).\n",
        "    @param theta: np.ndarray\n",
        "        Parameter vector for linear regression (shape: n,).\n",
        "\n",
        "    @return: float\n",
        "        The computed cost value as a single float.\n",
        "    \"\"\"\n",
        "    # get the number of training examples\n",
        "    m = len(y)\n",
        "\n",
        "    # Compute the predictions using the linear model\n",
        "    h_o = X @ theta  # ou np.dot(X, theta)\n",
        "\n",
        "    # Compute the error vector\n",
        "    errors = h_o - y\n",
        "\n",
        "    # Compute the cost as the mean squared error\n",
        "    J_o = (1/(2*m)) * np.sum(errors**2)\n",
        "\n",
        "    return J_o"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###gradient_descent.py"
      ],
      "metadata": {
        "id": "xudODrz11_PC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "@file gradient_descent.py\n",
        "@brief Implementa o algoritmo de descida do gradiente para regressão linear.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def gradient_descent(X, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Executa a descida do gradiente para minimizar a função de custo J(θ)\n",
        "    no contexto de regressão linear.\n",
        "\n",
        "    A cada iteração, os parâmetros theta são atualizados com base\n",
        "    no gradiente da função de custo em relação aos parâmetros atuais.\n",
        "\n",
        "    @param X: np.ndarray\n",
        "        Matriz de entrada (m amostras × n atributos), incluindo termo de bias.\n",
        "    @param y: np.ndarray\n",
        "        Vetor de saída esperado com dimensão (m,).\n",
        "    @param theta: np.ndarray\n",
        "        Vetor de parâmetros inicial (n,).\n",
        "    @param alpha: float\n",
        "        Taxa de aprendizado (learning rate).\n",
        "    @param num_iters: int\n",
        "        Número de iterações da descida do gradiente.\n",
        "\n",
        "    @return: tuple[np.ndarray, np.ndarray]\n",
        "        theta: vetor otimizado de parâmetros (n,).\n",
        "        J_history: vetor com o histórico do valor da função de custo em cada iteração (num_iters,).\n",
        "        theta_history: parâmetros em cada iteração (num_iters+1, n).\n",
        "    \"\"\"\n",
        "    # Obtem o número de amostras\n",
        "    m = len(y)\n",
        "    # Inicializa o vetor de custo J_history para armazenar o custo em cada iteração com zeros\n",
        "    # O vetor J_history tem o mesmo tamanho que o número de iterações\n",
        "    J_history = np.zeros(num_iters)\n",
        "\n",
        "    # Inicializa o vetor theta_history para armazenar os parâmetros em cada iteração\n",
        "    # O vetor theta_history tem o tamanho (num_iters + 1, n)\n",
        "    # O vetor theta_history é inicializado com zeros\n",
        "    # num_iters é o número de iterações\n",
        "    # O +1 é para armazenar os parâmetros iniciais antes de começar as iterações\n",
        "    # n é o número de parâmetros (atributos) no vetor theta\n",
        "    # Em resumo, theta_history é uma matriz onde cada linha representa os parâmetros em uma iteração\n",
        "    # e a primeira linha (índice 0) contém os parâmetros theta iniciais\n",
        "    theta_history = np.zeros((num_iters + 1, len(theta)))\n",
        "\n",
        "    # Armazena os parâmetros iniciais no vetor theta_history\n",
        "    # Isso é útil para visualizar como os parâmetros evoluem ao longo das iterações\n",
        "    # e como eles convergem para os valores ótimos\n",
        "    # theta_history é uma matriz onde cada linha representa os parâmetros em uma iteração\n",
        "    # A primeira linha (índice 0) contém os parâmetros theta iniciais\n",
        "    # Isso permite acompanhar a evolução dos parâmetros ao longo do processo de otimização\n",
        "    # As demais linhas serão preenchidas com os parâmetros atualizados em cada iteração\n",
        "    theta_history[0] = theta\n",
        "\n",
        "    for i in range():\n",
        "        # Calcula as previsões (hipótese) com base nos parâmetros atuais\n",
        "        # A hipótese (predições) é calculada como o produto escalar entre a matriz de entrada X e o vetor de parâmetros theta\n",
        "        predictions = X @ theta\n",
        "\n",
        "        # Calcula o erro entre as previsões e os valores reais\n",
        "        # O erro é a diferença entre as previsões e os valores reais\n",
        "        # Isso fornece uma medida de quão longe as previsões estão dos valores reais\n",
        "        # O erro é usado para calcular o gradiente da função de custo\n",
        "        erro = predictions - y\n",
        "\n",
        "        # Calcula o gradiente da função de custo em relação a theta\n",
        "        # O gradiente é calculado como a média do erro multiplicado pela matriz de entrada X\n",
        "        # Isso fornece uma medida de quão sensível é a função de custo em relação a cada parâmetro\n",
        "        # O gradiente é um vetor que aponta na direção de maior aumento da função de custo\n",
        "        # Portanto, para minimizar a função de custo, os parâmetros devem ser ajustados na direção oposta ao gradiente\n",
        "        gradient = (1/m) * (X.T @ error)\n",
        "\n",
        "        # Atualiza os parâmetros theta\n",
        "        # O novo valor de theta é obtido subtraindo o produto da taxa de aprendizado\n",
        "        # pelo gradiente da função de custo em relação a theta\n",
        "        # Isso ajusta os parâmetros na direção oposta ao gradiente, minimizando a função de custo\n",
        "        theta = theta - alpha * gradient\n",
        "\n",
        "        # Armazena o custo da iteração atual para análise\n",
        "        J_history[i] = compute_cost(X, y, theta)\n",
        "\n",
        "        # Armazena os parâmetros theta da iteração atual para análise\n",
        "        # Isso permite visualizar como os parâmetros evoluem ao longo das iterações\n",
        "        # Isso pode ser útil para entender o comportamento do algoritmo de descida do gradiente\n",
        "        # e como os parâmetros convergem para os valores ótimos\n",
        "        theta_history[i + 1] = theta\n",
        "\n",
        "    return theta, J_history, theta_history"
      ],
      "metadata": {
        "id": "ve1YoG0l2Gn5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###plot_data.py"
      ],
      "metadata": {
        "id": "6XHy2hFg2LVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "@file plot_data.py\n",
        "@brief Plots the data points.\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data(x, y):\n",
        "    \"\"\"\n",
        "    @brief Plot training data as red crosses.\n",
        "\n",
        "    @param x np.ndarray Independent variable (population)\n",
        "    @param y np.ndarray Dependent variable (profit)\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(x, y, 'rx', markersize=5)\n",
        "    plt.xlabel('Population of City in 10,000s')\n",
        "    plt.ylabel('Profit in $10,000s')\n",
        "    plt.title('Training Data')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Z71M9u6L2K-p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###warm_up_exercise.py"
      ],
      "metadata": {
        "id": "y7qTzVVi2Tsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "@file warm_up_exercise.py\n",
        "@brief Returns a 5x5 identity matrix.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def warm_up_exercise1():\n",
        "    \"\"\"\n",
        "    @brief Create and return a 5x5 identity matrix.\n",
        "\n",
        "    @return np.ndarray Identity matrix (5x5)\n",
        "    \"\"\"\n",
        "    return np.eye(5)\n",
        "\n",
        "def warm_up_exercise2(m=5):\n",
        "    \"\"\"\n",
        "    @brief Cria um vetor coluna de 1s, utilizado como termo de bias (intercepto) em regressão linear.\n",
        "\n",
        "    @param m: int\n",
        "        Número de exemplos (linhas).\n",
        "\n",
        "    @return np.ndarray\n",
        "        Vetor de shape (m, 1) com todos os valores iguais a 1.\n",
        "    \"\"\"\n",
        "    return np.ones((m, 1))\n",
        "\n",
        "def warm_up_exercise3(x):\n",
        "    \"\"\"\n",
        "    @brief Adiciona uma coluna de 1s (bias) ao vetor de entrada x.\n",
        "\n",
        "    @param x: np.ndarray\n",
        "        Vetor unidimensional de shape (m,)\n",
        "\n",
        "    @return np.ndarray\n",
        "        Matriz de shape (m, 2), com a primeira coluna sendo 1s (bias) e a segunda os valores de x.\n",
        "    \"\"\"\n",
        "    # obtem o número de exemplos\n",
        "    m = x.shape[0]\n",
        "    # Garante que x é um vetor coluna usando reshape. Use np.reshape\n",
        "    x = x.reshape(m, 1)\n",
        "    # Adiciona uma coluna de 1s (bias) ao vetor x. Use np.ones para criar um vetor de 1s\n",
        "    bias = np.ones((m, 1))\n",
        "    # Concatena a coluna de 1s (bias) com o vetor x. Use np.hstack para concatenar horizontalmente e retorne\n",
        "    return np.hstack((bias, x))\n",
        "\n",
        "def warm_up_exercise4(X, theta):\n",
        "    \"\"\"\n",
        "    @brief Realiza a multiplicação matricial entre X e θ, simulando h(θ) = X @ θ.\n",
        "\n",
        "    @param X: np.ndarray\n",
        "        Matriz de entrada de shape (m, n)\n",
        "\n",
        "    @param theta: np.ndarray\n",
        "        Vetor de parâmetros de shape (n,)\n",
        "\n",
        "    @return np.ndarray\n",
        "        Vetor de predições (m,)\n",
        "    \"\"\"\n",
        "    # retorna o resultado da multiplicação matricial entre X e θ\n",
        "    return X @ theta\n",
        "\n",
        "def warm_up_exercise5(predictions, y):\n",
        "    \"\"\"\n",
        "    @brief Calcula o vetor de erros quadráticos (squared errors) entre as predições e os valores reais.\n",
        "\n",
        "    @param predictions: np.ndarray\n",
        "        Vetor de predições (m,)\n",
        "\n",
        "    @param y: np.ndarray\n",
        "        Vetor de valores reais (m,)\n",
        "\n",
        "    @return np.ndarray\n",
        "        Vetor com os erros quadráticos: (pred - y)^2\n",
        "    \"\"\"\n",
        "    # Calcula o vetor de erros quadráticos (squared errors) entre as predições e os valores reais\n",
        "    # O vetor de erros quadráticos é calculado como a diferença entre as predições e os valores reais\n",
        "    return (predictions - y) ** 2\n",
        "\n",
        "def warm_up_exercise6(errors):\n",
        "    \"\"\"\n",
        "    @brief Calcula o custo médio (mean cost) a partir dos erros quadráticos.\n",
        "\n",
        "    @param errors: np.ndarray\n",
        "        Vetor de erros quadráticos (m,)\n",
        "\n",
        "    @return float\n",
        "        Custo médio (mean cost)\n",
        "    \"\"\"\n",
        "    # O custo médio é calculado como a média dos erros quadráticos\n",
        "    # Obtenha usando np.mean e não esqueça de dividir por 2\n",
        "    return np.mean(errors) / 2\n",
        "\n",
        "def warm_up_exercise7(X, y, theta):\n",
        "    \"\"\"\n",
        "    @brief Calcula o custo médio (mean cost) para um modelo de regressão linear.\n",
        "\n",
        "    @param X: np.ndarray\n",
        "        Matriz de entrada de shape (m, n)\n",
        "\n",
        "    @param y: np.ndarray\n",
        "        Vetor de valores reais (m,)\n",
        "\n",
        "    @param theta: np.ndarray\n",
        "        Vetor de parâmetros de shape (n,)\n",
        "\n",
        "    @return float\n",
        "        Custo médio (mean cost)\n",
        "    \"\"\"\n",
        "    # Use as funções auxiliares para calcular o custo médio\n",
        "    # 1. Calcule as predições usando a função warm_up_exercise4\n",
        "    # 2. Calcule os erros quadráticos usando a função warm_up_exercise5\n",
        "    # 3. Calcule o custo médio usando a função warm_up_exercise6\n",
        "    # 4. Retorne o custo médio\n",
        "    predictions = warm_up_exercise4(X, theta)\n",
        "    errors = warm_up_exercise5(predictions, y)\n",
        "    return warm_up_exercise6(errors)"
      ],
      "metadata": {
        "id": "NHt4jYK82TZB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###regressao-linear-ex1.py"
      ],
      "metadata": {
        "id": "GMG2BNw02bzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "@file regressao-linear-ex1.py\n",
        "@brief Exercise 2 - Linear Regression implementation with visualization.\n",
        "\n",
        "This script performs the following tasks:\n",
        "1. Runs a warm-up exercise.\n",
        "2. Loads and plots training data.\n",
        "3. Implements cost function and gradient descent.\n",
        "4. Predicts values for new inputs.\n",
        "5. Visualizes the cost function surface and contour.\n",
        "\n",
        "@author Teacher Thales Levi Azevedo Valente\n",
        "@subject Foundations of Neural Networks\n",
        "@course Computer Engineering\n",
        "@university Federal University of Maranhão\n",
        "@date 2025\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    @brief Executa todos os passos do exercício de regressão linear.\n",
        "\n",
        "    Esta função serve como ponto de partida para o exercício completo de regressão linear.\n",
        "    Ela executa uma série de etapas fundamentais, utilizadas como base para o aprendizado\n",
        "    de modelos supervisionados em redes neurais.\n",
        "\n",
        "    As principais etapas executadas são:\n",
        "      1. Executa o exercício de aquecimento (warm-up), imprimindo uma matriz identidade 5x5.\n",
        "      2. Carrega e plota os dados de treinamento de uma regressão linear simples.\n",
        "      3. Calcula o custo com diferentes valores de theta usando a função de custo J(θ).\n",
        "      4. Executa o algoritmo de descida do gradiente para minimizar a função de custo.\n",
        "      5. Plota a linha de regressão ajustada sobre os dados originais.\n",
        "      6. Realiza previsões para valores populacionais de 35.000 e 70.000.\n",
        "      7. Visualiza a função de custo J(θ₀, θ₁) em gráfico de superfície 3D e gráfico de contorno.\n",
        "\n",
        "    @instructions\n",
        "    - Os alunos devem garantir que todas as funções auxiliares estejam implementadas corretamente:\n",
        "        * warm_up_exercise()\n",
        "        * plot_data()\n",
        "        * compute_cost()\n",
        "        * gradient_descent()\n",
        "    - Todas as funções devem seguir padrão PEP8 e possuir docstrings no formato Doxygen.\n",
        "    - O script deve ser executado a partir do `main()`.\n",
        "\n",
        "    @note\n",
        "    O dataset de entrada `ex1data1.txt` deve estar no mesmo diretório Data.\n",
        "    A estrutura esperada dos dados é: [population, profit].\n",
        "\n",
        "    @return None\n",
        "    \"\"\"\n",
        "\n",
        "    # Garante que a pasta de figuras existe\n",
        "    os.makedirs(\"Figures\", exist_ok=True)\n",
        "\n",
        "    print('Executando o exercício de aquecimento (warm_up_exercise)...')\n",
        "    print('Matriz identidade 5x5:')\n",
        "    # Executa a função de aquecimento\n",
        "    # Essa função deve retornar uma matriz identidade 5x5\n",
        "    # representada como um array do NumPy.\n",
        "    # A função está definida em Functions/warm_up_exercise.py\n",
        "    # e foi importada no início deste arquivo.\n",
        "    print('Executando os exercícios de aquecimento...')\n",
        "\n",
        "    # Exercício 1: Matriz identidade 5x5\n",
        "    print('\\nExercício 1: Matriz identidade 5x5')\n",
        "    print(warm_up_exercise1()) # Esperado: [[1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1]]\n",
        "\n",
        "    # Exercício 2: Vetor coluna de 1s\n",
        "    print('\\nExercício 2: Vetor de 1s (m=3)')\n",
        "    print(warm_up_exercise2(3)) # Esperado: [[1], [1], [1]]\n",
        "\n",
        "    # Exercício 3: Adiciona bias ao vetor x\n",
        "    print('\\nExercício 3: Adiciona coluna de 1s ao vetor [2, 4, 6]')\n",
        "    x_ex3 = np.array([2, 4, 6])\n",
        "    print(warm_up_exercise3(x_ex3)) # Esperado: [[1, 2], [1, 4], [1, 6]]\n",
        "\n",
        "    # Exercício 4: Produto matricial X @ theta\n",
        "    print('\\nExercício 4: Produto X @ theta')\n",
        "    X_ex4 = warm_up_exercise3(x_ex3)\n",
        "    theta_ex4 = np.array([1, 2])\n",
        "    print(warm_up_exercise4(X_ex4, theta_ex4))  # Esperado: [5, 9, 13]\n",
        "\n",
        "    # Exercício 5: Erros quadráticos\n",
        "    print('\\nExercício 5: Erros quadráticos entre predições e y')\n",
        "    preds = warm_up_exercise4(X_ex4, theta_ex4)\n",
        "    y_ex5 = np.array([5, 9, 13])\n",
        "    print(warm_up_exercise5(preds, y_ex5))  # Esperado: [0, 0, 0]\n",
        "\n",
        "    # Exercício 6: Custo médio a partir dos erros\n",
        "    print('\\nExercício 6: Custo médio')\n",
        "    errors_ex6 = warm_up_exercise5(preds, y_ex5)\n",
        "    print(warm_up_exercise6(errors_ex6))  # Esperado: 0.0\n",
        "\n",
        "    # Exercício 7: Custo médio com base em X, y e theta\n",
        "    print('\\nExercício 7: Cálculo do custo médio completo')\n",
        "    print(warm_up_exercise7(X_ex4, y_ex5, theta_ex4))  # Esperado: 0.0\n",
        "\n",
        "\n",
        "    input(\"Programa pausado. Pressione Enter para continuar...\")\n",
        "\n",
        "    print('Plotando os dados...')\n",
        "    # Carrega os dados de treinamento a partir do arquivo ex1data1.txt\n",
        "    # O arquivo contém duas colunas: a primeira com a população da cidade\n",
        "    # (em dezenas de milhar) e a segunda com o lucro (em dezenas de mil dólares).\n",
        "    # Os dados são carregados usando a função np.loadtxt do NumPy.\n",
        "    # A função np.loadtxt lê os dados do arquivo e os armazena em um array NumPy.\n",
        "    data = np.loadtxt('Data/ex1data1.txt', delimiter=',')\n",
        "    # Separa os dados em duas variáveis: x e y\n",
        "    # x contém a população da cidade (em dezenas de milhar)\n",
        "    # y contém o lucro (em dezenas de mil dólares)\n",
        "    # A primeira coluna de data é a população (x), a feature\n",
        "    # que será usada para prever o lucro.\n",
        "    x = data[:, 0]\n",
        "    # A segunda coluna de data é o lucro (y), a label ou target\n",
        "    y = data[:, 1]\n",
        "    # Agora, obtemos o número de exemplos de treinamento (m)\n",
        "    m = len(y)\n",
        "\n",
        "    # Plotagem dos dados\n",
        "    # Utiliza a função plot_data para exibir os pontos (x, y) em um gráfico 2D.\n",
        "    # A função está definida em Functions/plot_data.py\n",
        "    # e foi importada no início do arquivo.\n",
        "    plot_data(x, y)\n",
        "\n",
        "    input(\"Programa pausado. Pressione Enter para continuar...\")\n",
        "\n",
        "    # Preparação dos dados para o algoritmo de descida do gradiente\n",
        "    # Adiciona uma coluna de 1s à matriz x para representar o termo de interceptação (bias).\n",
        "    # Isso é feito com np.column_stack, combinando uma coluna de 1s com os valores de x.\n",
        "    # A nova matriz x_aug terá duas colunas: a primeira com 1s e a segunda com os valores originais de x.\n",
        "    x_aug = np.column_stack((np.ones(m), x))\n",
        "\n",
        "    # Inicialização de theta como um vetor nulo (vetor de zeros)\n",
        "    # Inicializa o vetor de parâmetros theta como um vetor nulo com 2 elementos (theta[0] e theta[1]).\n",
        "    # O primeiro elemento representa o intercepto (bias) e o segundo o coeficiente angular (inclinação).\n",
        "    # Esse vetor será ajustado durante a execução do algoritmo de descida do gradiente.\n",
        "    theta = np.zeros(2)\n",
        "\n",
        "    # Parâmetros da descida do gradiente\n",
        "    # Define o número de iterações e a taxa de aprendizado (alpha)\n",
        "    # O número de iterações determina quantas vezes os parâmetros serão atualizados.\n",
        "    iterations = 1500\n",
        "\n",
        "    # A taxa de aprendizado (alpha) controla o tamanho do passo dado em cada iteração do algoritmo de descida do gradiente.\n",
        "    # Um alpha muito grande pode fazer o algoritmo divergir, enquanto um muito pequeno pode torná-lo lento.\n",
        "    # Aqui, alpha é definido como 0.01, um valor comumente usado em problemas de regressão linear.\n",
        "    # Você pode experimentar outros valores para ver como o algoritmo se comporta.\n",
        "    alpha = 0.01\n",
        "\n",
        "    print('\\nTestando a função de custo...')\n",
        "    # Utiliza a função compute_cost para calcular o custo com os parâmetros iniciais (theta = [0, 0]).\n",
        "    # Essa função mede o quão bem os parâmetros atuais se ajustam aos dados de treinamento.\n",
        "    # Ela está definida em Functions/compute_cost.py e foi importada anteriormente.\n",
        "    # Os parâmetros de entrada são a matriz x_aug (com 1s e valores de x), o vetor y (lucro) e o vetor theta (parâmetros).\n",
        "    cost = compute_cost(x_aug, y, theta)\n",
        "    print(f'Com theta = [0 ; 0]\\nCusto calculado = {cost:.2f}')\n",
        "    print('Valor esperado para o custo (aproximadamente): 32.07')\n",
        "\n",
        "    # Testando a função de custo com outro valor de theta\n",
        "    # Aqui, testamos a função de custo com um valor diferente de theta ([-1, 2]).\n",
        "    # Isso nos permite verificar se a função de custo está funcionando corretamente.\n",
        "    # O valor de theta = [-1, 2] é um exemplo e não representa o ajuste ideal.\n",
        "    cost = compute_cost(x_aug, y, np.array([-1, 2]))\n",
        "    print(f'\\nCom theta = [-1 ; 2]\\nCusto calculado = {cost:.2f}')\n",
        "    print('Valor esperado para o custo (aproximadamente): 54.24')\n",
        "\n",
        "    input(\"Programa pausado. Pressione Enter para continuar...\")\n",
        "\n",
        "    print('\\nExecutando a descida do gradiente...')\n",
        "    # Executa o algoritmo de descida do gradiente para encontrar os parâmetros ótimos (theta).\n",
        "    # A função gradient_descent é definida em Functions/gradient_descent.py\n",
        "    # e foi importada anteriormente.\n",
        "    # Ela recebe como parâmetros a matriz x_aug (com 1s e valores de x), o vetor y (lucro),\n",
        "\n",
        "    # Após os testes, inicializamos os parâmetros theta com valores diferentes de zero.\n",
        "    # theta = [8.5, 4.0] é o ponto de partida padrão. Foi estabelecido empiricamente ao olhar os gráficos.\n",
        "    # Você pode experimentar outros valores para ver como o algoritmo se comporta.\n",
        "    theta = np.array([8.5, 4.0])\n",
        "    # o vetor theta, a taxa de aprendizado (alpha) e o número de iterações.\n",
        "    # A função retorna os parâmetros ajustados (theta), o histórico de custos (J_history) e o histórico de theta (theta_history).\n",
        "    # O histórico de custos é usado para visualizar a convergência do algoritmo.\n",
        "    # O histórico de theta é usado para visualizar a trajetória do gradiente na superfície da função de custo.\n",
        "    theta, J_history, theta_history = gradient_descent(x_aug, y, theta, alpha, iterations)\n",
        "\n",
        "    print('Parâmetros theta encontrados pela descida do gradiente:')\n",
        "    print(theta)\n",
        "    print('Valores esperados para theta (aproximadamente):')\n",
        "    print(' -3.6303\\n  1.1664')\n",
        "\n",
        "    # 1. Gráfico da convergência da função de custo\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(np.arange(1, iterations + 1), J_history, 'b-', linewidth=2)\n",
        "    plt.xlabel('Iteração')\n",
        "    plt.ylabel('Custo J(θ)')\n",
        "    plt.title('Convergência da Descida do Gradiente')\n",
        "    plt.savefig(\"Figures/convergencia_custo.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(\"Figures/convergencia_custo.svg\", format='svg', bbox_inches='tight')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Gráfico do Ajuste da Regressão Linear\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(x, y, 'rx', markersize=5, label='Dados de treino')\n",
        "    plt.plot(x, x_aug @ theta, 'b-', linewidth=2, label='Regressão linear')\n",
        "    plt.xlabel('População da cidade (em dezenas de mil)')\n",
        "    plt.ylabel('Lucro (em dezenas de mil dólares)')\n",
        "    plt.title('Ajuste da Regressão Linear')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Previsões usando theta ajustado\n",
        "    predict1 = np.array([1, 3.5]) @ theta\n",
        "    predict2 = np.array([1, 7.0]) @ theta\n",
        "    print(f'\\nPara população = 35.000, lucro previsto = ${predict1 * 10000:.2f}')\n",
        "    print(f'Para população = 70.000, lucro previsto = ${predict2 * 10000:.2f}')\n",
        "    input(\"Programa pausado. Pressione Enter para continuar...\")\n",
        "\n",
        "    # 3. Gráfico de superfície 3D da função de custo J(θ₀, θ₁)\n",
        "    print('Visualizando a função J(theta_0, theta_1) – superfície 3D...')\n",
        "    theta0_vals = np.linspace(-10, 10, 100)\n",
        "    theta1_vals = np.linspace(-1, 4, 100)\n",
        "    j_vals = np.zeros((len(theta0_vals), len(theta1_vals)))\n",
        "    for i, t0 in enumerate(theta0_vals):\n",
        "        for j, t1 in enumerate(theta1_vals):\n",
        "            j_vals[i, j] = compute_cost(x_aug, y, np.array([t0, t1]))\n",
        "    j_vals = j_vals.T\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    t0_mesh, t1_mesh = np.meshgrid(theta0_vals, theta1_vals)\n",
        "    ax.plot_surface(t0_mesh, t1_mesh, j_vals, cmap='viridis', edgecolor='none')\n",
        "    ax.set_xlabel(r'$\\theta_0$')\n",
        "    ax.set_ylabel(r'$\\theta_1$')\n",
        "    ax.set_zlabel('Custo')\n",
        "    plt.title('Superfície da Função de Custo')\n",
        "    plt.show()\n",
        "\n",
        "    # 4. Gráfico de contorno da função de custo\n",
        "    print('Visualizando a função J(theta_0, theta_1) – contorno...')\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.contour(theta0_vals, theta1_vals, j_vals, levels=np.logspace(-2, 3, 20))\n",
        "    plt.plot(theta[0], theta[1], 'rx', markersize=10, linewidth=2)\n",
        "    plt.xlabel(r'$\\theta_0$')\n",
        "    plt.ylabel(r'$\\theta_1$')\n",
        "    plt.title('Contorno da Função de Custo')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # 5) Contorno da função de custo + trajetória do gradiente\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    # desenha as linhas de contorno\n",
        "    cs = plt.contour(theta0_vals, theta1_vals, j_vals,\n",
        "                     levels=np.logspace(-2, 3, 20))\n",
        "    plt.clabel(cs, inline=1, fontsize=8)  # mostra valores de custo nas linhas\n",
        "\n",
        "    # sobrepõe a trajetória dos thetas\n",
        "    plt.plot(theta_history[:, 0], theta_history[:, 1],\n",
        "             'r.-', markersize=6, label='Trajetória do gradiente')\n",
        "\n",
        "    plt.xlabel(r'$\\theta_0$')\n",
        "    plt.ylabel(r'$\\theta_1$')\n",
        "    plt.title('Contorno da Função de Custo com Trajetória')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(\"Figures/contorno_trajetoria.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(\"Figures/contorno_trajetoria.svg\", format='svg', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 7) Superfície da função de custo com trajetória 3D melhor visualizada\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # 7.1 Plota a superfície semitransparente\n",
        "    surf = ax.plot_surface(\n",
        "        t0_mesh, t1_mesh, j_vals,\n",
        "        cmap='viridis',\n",
        "        edgecolor='none',\n",
        "        alpha=0.6       # deixa a superfície semitransparente\n",
        "    )\n",
        "\n",
        "    # 7.2 Ajusta ângulo de visão\n",
        "    ax.view_init(elev=18, azim=-18, roll=-5)\n",
        "\n",
        "    # 7.3 Trajetória do gradiente em linha vermelha grossa\n",
        "    costs = np.concatenate(\n",
        "        ([compute_cost(x_aug, y, theta_history[0])], J_history)\n",
        "    )\n",
        "\n",
        "    # Inserindo a trajetória 3D do gradiente\n",
        "    # theta_history: shape (iter+1, 2), J_history: shape (iter,)\n",
        "    ax.plot(\n",
        "        theta_history[:, 0],\n",
        "        theta_history[:, 1], costs,\n",
        "        color='red',\n",
        "        linewidth=3,\n",
        "        marker='o',\n",
        "        markersize=4,\n",
        "        label='Trajetória do gradiente'\n",
        "    )\n",
        "\n",
        "    # 7.4 Destacar ponto inicial e final\n",
        "    ax.scatter(\n",
        "        theta_history[0, 0], theta_history[0, 1], costs[0],\n",
        "        color='blue', s=50, label='Início'\n",
        "    )\n",
        "    ax.scatter(\n",
        "        theta_history[-1, 0], theta_history[-1, 1], costs[-1],\n",
        "        color='green', s=50, label='Convergência'\n",
        "    )\n",
        "\n",
        "    ax.set_xlabel(r'$\\theta_0$')\n",
        "    ax.set_ylabel(r'$\\theta_1$')\n",
        "    ax.set_zlabel('Custo')\n",
        "    plt.title('Superfície da Função de Custo com Trajetória 3D')\n",
        "    ax.legend()\n",
        "    plt.savefig(\"Figures/superficie_trajetoria.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(\"Figures/superficie_trajetoria.svg\", format='svg', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "rg0qUTum2bpJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}